{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First iteration of modeling, going through several different classification algorithms. Of these, Gradient Boosted Classifier using unscaled data worked the best. \n",
    "## For reference/example only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "%matplotlib notebook\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence, partial_dependence\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data\n",
    "\n",
    "We did not share our modeling data, so you will have to create your own. The pipeline tool can help you do this. If you save the results to a csv, `masterdf_rrt` and `masterdf_nonrrt` are dataframes with the modeling data for each of the positive and negative classes, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masterdf_rrt = pd.read_csv('RRT_modeling_table_13hr_raw.csv')\n",
    "masterdf_nonrrt = pd.read_csv('NonRRT_modeling_table_13hr_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_use = ['on_iv', 'bu-nal',\n",
    "           'age', 'sex', 'obese', 'smoker', 'prev_rrt',\n",
    "           'DBP_mean', 'DBP_recent', # take the mean of all the measurements & the most recently observed point\n",
    "            'SBP_mean', 'SBP_recent',\n",
    "            'MAP_mean', 'MAP_recent', # mean arterial pressure\n",
    "             'temp_mean', 'temp_recent',# temperature\n",
    "             'SPO2_mean', 'SPO2_recent',\n",
    "            'RR_mean', 'RR_recent', # respiratory rate\n",
    "            'pulse_mean', 'pulse_recent',\n",
    "           'anticoagulants', 'narcotics', 'narc-ans', #narcotic analgesics\n",
    "            'antipsychotics', 'chemo'\n",
    "]\n",
    "\n",
    "colsfornan = ['DBP_mean', 'DBP_recent', # take the mean of all the measurements & the most recently observed point\n",
    "            'SBP_mean', 'SBP_recent',\n",
    "            'MAP_mean', 'MAP_recent', # mean arterial pressure\n",
    "             'temp_mean', 'temp_recent',# temperature\n",
    "             'SPO2_mean', 'SPO2_recent',\n",
    "            'RR_mean', 'RR_recent', # respiratory rate\n",
    "            'pulse_mean', 'pulse_recent']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take out: rrt_ce_id, encntr_id, event_end_dt_tm, timestart, timeend,\n",
    "# RASS score, GCS score, HR, CO2, O2\n",
    "X_rrt = masterdf_rrt[col_use]\n",
    "\n",
    "# if 'obese' is Nan, then set the patient to be not obese.\n",
    "obesenanmask = np.where(pd.isnull(X_rrt['obese']))[0]\n",
    "X_rrt.loc[obesenanmask, 'obese'] = 0\n",
    "\n",
    "# Write out rows that are not all 0/NaNs across. (if all nans, remove this sample)\n",
    "X_rrt = X_rrt.loc[np.where(X_rrt.ix[:, colsfornan].sum(axis=1, skipna=True)!=0)[0]]\n",
    "\n",
    "#reset index\n",
    "X_rrt = X_rrt.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take out: encntr_id, not_rrt_time, timestart, timeend,\n",
    "#  RASS score, GCS score, HR, CO2, O2\n",
    "X_notrrt = masterdf_nonrrt[col_use]\n",
    "\n",
    "# if 'obese' is Nan, then set the patient to be not obese.\n",
    "obesenanmask = np.where(pd.isnull(X_notrrt['obese']))[0]\n",
    "X_notrrt.loc[obesenanmask, 'obese'] = 0\n",
    "\n",
    "# Write out rows that are NOT all 0/NaNs across.\n",
    "X_notrrt = X_notrrt.iloc[np.where(X_notrrt.ix[:, colsfornan].sum(axis=1, skipna=True)!=0)[0]]\n",
    "\n",
    "#reset index\n",
    "X_notrrt = X_notrrt.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make sure to reset index if haven't previously -- I did not run this before saving processed .p files\n",
    "X_notrrt = X_notrrt.reset_index(drop=True)\n",
    "X_rrt = X_rrt.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# additional for non-rrt: drop samples with lots of NaNs since we have plenty of samples\n",
    "# DROP THE ROWS WHERE PULSE IS NAN\n",
    "X_notrrt = X_notrrt.ix[np.where(pd.isnull(X_notrrt['pulse_mean'])!=True)[0]]\n",
    "X_notrrt = X_notrrt.reset_index(drop=True)\n",
    "\n",
    "# Do a similar thing for all rows with significant nans:\n",
    "X_notrrt = X_notrrt.ix[np.where(pd.isnull(X_notrrt['RR_mean'])!=True)[0]]\n",
    "X_notrrt = X_notrrt.reset_index(drop=True)\n",
    "X_notrrt = X_notrrt.ix[np.where(pd.isnull(X_notrrt['MAP_mean'])!=True)[0]]\n",
    "X_notrrt = X_notrrt.reset_index(drop=True)\n",
    "X_notrrt = X_notrrt.ix[np.where(pd.isnull(X_notrrt['temp_mean'])!=True)[0]]\n",
    "X_notrrt = X_notrrt.reset_index(drop=True)\n",
    "X_notrrt = X_notrrt.ix[np.where(pd.isnull(X_notrrt['SPO2_mean'])!=True)[0]]\n",
    "X_notrrt = X_notrrt.reset_index(drop=True)\n",
    "\n",
    "# add labels to indicate positive or negative class\n",
    "X_rrt['label'] = 1\n",
    "X_notrrt['label'] = 0\n",
    "\n",
    "# Combine the tables\n",
    "XY = pd.concat([X_rrt, X_notrrt])\n",
    "XY = XY.reset_index(drop=True)\n",
    "y = XY.pop('label')\n",
    "X = XY\n",
    "\n",
    "# Fill nans with mean of columns\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "# map genders to 1/0\n",
    "X['is_male'] = X['sex'].map({'M': 1, 'F': 0})\n",
    "X.pop('sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_printout(X_test, y_test, fittedModel):\n",
    "    print \"AUC-ROC Score of model: \", roc_auc_score(y_test, fittedModel.predict_proba(X_test)[:,1])\n",
    "    print \"Precision Score of model: \", precision_score(y_test, fittedModel.predict(X_test))\n",
    "    print \"Recall Score of model: \", recall_score(y_test, fittedModel.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrCV = LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
    "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
    "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=1,\n",
    "           refit=True, scoring='roc_auc', solver='lbfgs', tol=0.0001, verbose=0)\n",
    "lrCV.fit(X, y)\n",
    "lrCV.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try different solver\n",
    "lrCV = LogisticRegressionCV(Cs=5, class_weight=None, cv=5, dual=False,\n",
    "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
    "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=1,\n",
    "           refit=True, scoring='roc_auc', solver='liblinear', tol=0.0001, verbose=0)\n",
    "lrCV.fit(X_train, y_train)\n",
    "lrCV.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrCV.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrCV.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrCV.Cs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_printout(X_test, y_test, lrCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, lrCV.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare to traditional LR..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
    "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_printout(X_test, y_test, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print classification_report(y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR & LRCV returned essentially the same results!\n",
    "### Let's rerun LR with scaled data. And then calculate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xscaled = StandardScaler().fit_transform(X)\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(Xscaled, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrs = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
    "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "lrs.fit(Xs_train, ys_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_printout(Xs_test, ys_test, lrs)\n",
    "print classification_report(ys_test, lrs.predict(Xs_test))\n",
    "confusion_matrix(ys_test, lrs.predict(Xs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled version performs (slightly) better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfs = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "rfs.fit(Xs_train, ys_train)\n",
    "score_printout(Xs_test, ys_test, rfs)\n",
    "print classification_report(ys_test, rfs.predict(Xs_test))\n",
    "confusion_matrix(ys_test, rfs.predict(Xs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "rf.fit(X_train, y_train)\n",
    "score_printout(X_test, y_test, rf)\n",
    "print classification_report(y_test, rf.predict(X_test))\n",
    "confusion_matrix(y_test, rf.predict(X_test))\n",
    "# scaled & unscaled random forest looks very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfs = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "rfs.fit(Xs_train, ys_train)\n",
    "score_printout(Xs_test, ys_test, rfs)\n",
    "print classification_report(ys_test, rfs.predict(Xs_test))\n",
    "confusion_matrix(ys_test, rfs.predict(Xs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Increase # estimators\n",
    "# Note, turning oob score on makes the result worse...\n",
    "rfs = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "rfs.fit(Xs_train, ys_train)\n",
    "score_printout(Xs_test, ys_test, rfs)\n",
    "print classification_report(ys_test, rfs.predict(Xs_test))\n",
    "confusion_matrix(ys_test, rfs.predict(Xs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (kernel: sigmoid does not work, rbf appears best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svms =  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "svms.fit(Xs_train, ys_train)\n",
    "score_printout(Xs_test, ys_test, svms)\n",
    "print classification_report(ys_test, svms.predict(Xs_test))\n",
    "confusion_matrix(ys_test, svms.predict(Xs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svms =  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape=None, degree=3, gamma='auto', kernel='poly',\n",
    "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "svms.fit(Xs_train, ys_train)\n",
    "score_printout(Xs_test, ys_test, svms)\n",
    "print classification_report(ys_test, svms.predict(Xs_test))\n",
    "confusion_matrix(ys_test, svms.predict(Xs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svms =  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
    "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "svms.fit(Xs_train, ys_train)\n",
    "score_printout(Xs_test, ys_test, svms)\n",
    "print classification_report(ys_test, svms.predict(Xs_test))\n",
    "confusion_matrix(ys_test, svms.predict(Xs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svms =  SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "svms.fit(Xs_train, ys_train)\n",
    "score_printout(Xs_test, ys_test, svms)\n",
    "print classification_report(ys_test, svms.predict(Xs_test))\n",
    "confusion_matrix(ys_test, svms.predict(Xs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldas = LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
    "              solver='svd', store_covariance=False, tol=0.0001)\n",
    "ldas.fit(Xs_train, ys_train)\n",
    "score_printout(Xs_test, ys_test, ldas)\n",
    "print classification_report(ys_test, ldas.predict(Xs_test))\n",
    "confusion_matrix(ys_test, ldas.predict(Xs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldas = LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage='auto',\n",
    "              solver='eigen', store_covariance=False, tol=0.0001)\n",
    "ldas.fit(Xs_train, ys_train)\n",
    "score_printout(Xs_test, ys_test, ldas)\n",
    "print classification_report(ys_test, ldas.predict(Xs_test))\n",
    "confusion_matrix(ys_test, ldas.predict(Xs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting (with partial dependence plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbcs = GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
    "              min_samples_leaf=1, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "              presort='auto', random_state=1, subsample=1.0, verbose=0,\n",
    "              warm_start=False)\n",
    "gbcs.fit(Xs_train, ys_train)\n",
    "score_printout(Xs_test, ys_test, gbcs)\n",
    "print classification_report(ys_test, gbcs.predict(Xs_test))\n",
    "confusion_matrix(ys_test, gbcs.predict(Xs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# changed subsampling: Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias.\n",
    "gbcs = GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
    "              min_samples_leaf=1, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "              presort='auto', random_state=1, subsample=0.75, verbose=0,\n",
    "              warm_start=False)\n",
    "gbcs.fit(Xs_train, ys_train)\n",
    "score_printout(Xs_test, ys_test, gbcs)\n",
    "print classification_report(ys_test, gbcs.predict(Xs_test))\n",
    "confusion_matrix(ys_test, gbcs.predict(Xs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tried changing loss to exponential --> worse.\n",
    "# change max features to sqrt --> worse.\n",
    "# upped min_samples_split, from 2 to 3, improved. (other values did not cause improvement)\n",
    "\n",
    "gbcs = GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
    "              min_samples_leaf=1, min_samples_split=3,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "              presort='auto', random_state=1, subsample=0.75, verbose=0,\n",
    "              warm_start=False)\n",
    "gbcs.fit(Xs_train, ys_train)\n",
    "score_printout(Xs_test, ys_test, gbcs)\n",
    "print classification_report(ys_test, gbcs.predict(Xs_test))\n",
    "confusion_matrix(ys_test, gbcs.predict(Xs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, n in enumerate(X.columns.get_values()):\n",
    "    print i, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_partial_dependence(gbcs, Xs_train, range(0, 6, 1), feature_names=X.columns.get_values(), n_jobs=3, grid_resolution=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_partial_dependence(gbcs, Xs_train, range(6, 12, 1), feature_names=X.columns.get_values(), n_jobs=3, grid_resolution=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_partial_dependence(gbcs, Xs_train, range(12, 18, 1), feature_names=X.columns.get_values(), n_jobs=-1, grid_resolution=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_partial_dependence(gbcs, Xs_train, range(18, 26, 1), feature_names=X.columns.get_values(), n_jobs=-1, grid_resolution=100)\n",
    "plt.subplots_adjust(top=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GBC without scaling works better than GBC with. Hmm.\n",
    "gbc = GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
    "              min_samples_leaf=1, min_samples_split=3,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "              presort='auto', random_state=1, subsample=0.75, verbose=0,\n",
    "              warm_start=False)\n",
    "gbc.fit(X_train, y_train)\n",
    "score_printout(X_test, y_test, gbc)\n",
    "print classification_report(y_test, gbc.predict(X_test))\n",
    "confusion_matrix(y_test, gbc.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_partial_dependence(gbc, X_train, range(0, 6, 1), feature_names=X.columns.get_values(), n_jobs=3, grid_resolution=50)\n",
    "plt.subplots_adjust(top=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_partial_dependence(gbc, X_train, range(6, 12, 1), feature_names=X.columns.get_values(), n_jobs=3, grid_resolution=50)\n",
    "plt.subplots_adjust(top=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_partial_dependence(gbc, X_train, range(12, 18, 1), feature_names=X.columns.get_values(), n_jobs=3, grid_resolution=50)\n",
    "plt.subplots_adjust(top=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_partial_dependence(gbc, X_train, range(18, 26, 1), feature_names=X.columns.get_values(), n_jobs=3, grid_resolution=50)\n",
    "plt.subplots_adjust(top=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbcRankedFeatures = sorted(zip(X.columns, gbc.feature_importances_), \n",
    "                          key=lambda pair: pair[1], \n",
    "                          reverse=True)\n",
    "\n",
    "def make_feature_importance_plot(featuresAndImportances, numFeatures):\n",
    "    topN = featuresAndImportances[:numFeatures]\n",
    "    labels = [pair[0] for pair in topN]\n",
    "    values = [pair[1] for pair in topN]\n",
    "    ind = np.arange(len(values))\n",
    "    width = 0.35   \n",
    "    plt.bar(range(numFeatures), values, width=0.8)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_xticks(ind+width)\n",
    "    ax.set_xticklabels(labels, rotation=60, size=12)\n",
    "    plt.xlabel('Feature', size=20)\n",
    "    plt.ylabel('Importance', size=20)\n",
    "    plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "make_feature_importance_plot(gbcRankedFeatures, 26)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbc.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search for best GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GBC without scaling works better than GBC with. Hmm.\n",
    "gbc = GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
    "              min_samples_leaf=1, min_samples_split=3,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "              presort='auto', random_state=1, subsample=0.75, verbose=0,\n",
    "              warm_start=False)\n",
    "gbc.fit(X_train, y_train)\n",
    "score_printout(X_test, y_test, gbc)\n",
    "print classification_report(y_test, gbc.predict(X_test))\n",
    "confusion_matrix(y_test, gbc.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paramGrid = {'n_estimators': [100, 200, 300],\n",
    "             'learning_rate': [0.1, 0.5, 0.75, 0.2],\n",
    "             'max_depth': [3, 4, 5, 6],\n",
    "             'min_samples_leaf': [1, 2],\n",
    "             'subsample': [0.75],\n",
    "             'loss': ['deviance'],\n",
    "             'max_features': [None, 'auto']\n",
    "            }\n",
    "\n",
    "gs = GridSearchCV(GradientBoostingClassifier(), \n",
    "                  param_grid=paramGrid, \n",
    "                  scoring='roc_auc', \n",
    "                  n_jobs=-1, \n",
    "                  cv=5, \n",
    "                  verbose=10)\n",
    "\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model so far: GBC!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbcModel = GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
    "              max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
    "              min_samples_leaf=2, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
    "              presort='auto', random_state=None, subsample=0.75, verbose=0,\n",
    "              warm_start=False)\n",
    "gbcModel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbcRankedFeatures = sorted(zip(X.columns, gbcModel.feature_importances_), \n",
    "                          key=lambda pair: pair[1], \n",
    "                          reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_feature_importance_plot(featuresAndImportances, numFeatures):\n",
    "    topN = featuresAndImportances[:numFeatures]\n",
    "    labels = [pair[0] for pair in topN]\n",
    "    values = [pair[1] for pair in topN]\n",
    "    ind = np.arange(len(values))\n",
    "    width = 0.35   \n",
    "    plt.bar(range(numFeatures), values, width=0.8)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_xticks(ind+width)\n",
    "    ax.set_xticklabels(labels, rotation=60, size=12)\n",
    "    plt.xlabel('Feature', size=20)\n",
    "    plt.ylabel('Importance', size=20)\n",
    "    plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_printout(X_test, y_test, gbcModel)\n",
    "print classification_report(y_test, gbcModel.predict(X_test))\n",
    "confusion_matrix(y_test, gbcModel.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbcRankedFeatures = sorted(zip(X.columns, gbcModel.feature_importances_), \n",
    "                          key=lambda pair: pair[1], \n",
    "                          reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "make_feature_importance_plot(gbcRankedFeatures, 15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_partial_dependence(gbcModel, X_train, range(1, 6, 1), feature_names=X.columns.get_values(), n_jobs=3, grid_resolution=50)\n",
    "plt.subplots_adjust(top=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_partial_dependence(gbcModel, X_train, range(6, 12, 1), feature_names=X.columns.get_values(), n_jobs=3, grid_resolution=50)\n",
    "plt.subplots_adjust(top=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_partial_dependence(gbcModel, X_train, range(6, 8, 1), feature_names=X.columns.get_values(), n_jobs=3, grid_resolution=50)\n",
    "plt.subplots_adjust(top=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_partial_dependence(gbcModel, X_train, range(18, 24, 1), feature_names=X.columns.get_values(), n_jobs=3, grid_resolution=50)\n",
    "plt.subplots_adjust(top=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plot_partial_dependence(gbcModel, X_train, range(24, 26, 1), feature_names=X.columns.get_values(), n_jobs=3, grid_resolution=50)\n",
    "plt.subplots_adjust(top=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Use 3-D plot to investigate feature interactions for weak partial dependence plots... (weak effect may be masked by stronger interaction with other features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# not quite getting the results I was expecting. Needs work.\n",
    "fig = plt.figure()\n",
    "\n",
    "names = X_train.columns\n",
    "\n",
    "target_feature = (3, 21)\n",
    "pdp, (x_axis, y_axis) = partial_dependence(gbcModel, target_feature,\n",
    "                                           X=X_train, grid_resolution=50)\n",
    "XX, YY = np.meshgrid(x_axis, y_axis)\n",
    "Z = pdp.T.reshape(XX.shape).T\n",
    "ax = Axes3D(fig)\n",
    "surf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1, cmap=plt.cm.BuPu)\n",
    "ax.set_xlabel(names[target_feature[0]])\n",
    "ax.set_ylabel(names[target_feature[1]])\n",
    "ax.set_zlabel('Partial dependence')\n",
    "#  pretty init view\n",
    "ax.view_init(elev=22, azim=122)\n",
    "plt.colorbar(surf)\n",
    "plt.suptitle('')\n",
    "plt.subplots_adjust(top=0.9)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
